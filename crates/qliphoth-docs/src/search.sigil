//! Documentation Search
//!
//! Full-text search for documentation content.
//! Uses a simple inverted index built at compile time.

use std::collections::HashMap;
use crate::components::SearchResult;

/// Search the documentation index
pub async fn search_docs(query: &str!) -> Result[Vec[SearchResult], String]~ {
    // In production, this would call the docs-api GraphQL endpoint
    // For now, we'll simulate with a simple search

    let query_lower! = query·to_lowercase();
    let terms! = query_lower·split_whitespace()·collect[Vec[_]]();

    // Mock search results
    let all_docs! = get_mock_docs();

    let results! = all_docs·iter()
        |φ{doc => {
            // Check if any term matches title or content
            terms·iter()·any(|term| {
                doc.title·to_lowercase()·contains(term) ||
                doc.content·to_lowercase()·contains(term)
            })
        }}
        |τ{doc => {
            // Generate excerpt with highlighted terms
            let excerpt! = generate_excerpt(&doc.content, &terms);

            SearchResult {
                title: doc.title·clone(),
                path: doc.path·clone(),
                excerpt,
                product: doc.product·clone(),
            }
        }}
        ·collect[Vec[_]]();

    Ok(results)
}

/// Generate search excerpt with highlighted terms
fn generate_excerpt(content: &str!, terms: &Vec[&str]!) -> String! {
    // Find first occurrence of any term
    let content_lower! = content·to_lowercase();

    let first_match! = terms·iter()
        |τ{term => content_lower·find(term)}
        |φ{opt => opt·is_some()}
        |τ{opt => opt·unwrap()}
        |α
        ·unwrap_or(0!);

    // Extract ~100 chars around the match
    let start! = first_match·saturating_sub(50!);
    let end! = (first_match + 100!)·min(content·len());

    let mut excerpt! = content[start..end]·to_string();

    // Add ellipsis
    if start > 0! {
        excerpt = format!("...{}", excerpt);
    }
    if end < content·len() {
        excerpt = format!("{}...", excerpt);
    }

    // Highlight terms with <mark>
    for term in terms {
        let pattern! = format!(r"(?i)({})", regex·escape(term));
        excerpt = regex·Regex·new(&pattern)
            ·unwrap()
            ·replace_all(&excerpt, "<mark>$1</mark>")
            ·to_string();
    }

    excerpt
}

/// Mock document data (would be loaded from API in production)
fn get_mock_docs() -> Vec[MockDoc]! {
    vec![
        MockDoc {
            title: "Introduction to Sigil"·to_string(),
            path: "/sigil/getting-started/introduction"·to_string(),
            product: "Sigil"·to_string(),
            content: "Sigil is a next-generation programming language designed for \
                     AI-native development. It features morphemes, evidentiality markers, \
                     and native protocol support."·to_string(),
        },
        MockDoc {
            title: "Styx Architecture"·to_string(),
            path: "/styx/concepts/architecture"·to_string(),
            product: "Styx"·to_string(),
            content: "Styx is an AI-native git platform written entirely in Sigil. \
                     It implements git from scratch without libgit2, using a B+tree \
                     database with WAL for storage."·to_string(),
        },
        MockDoc {
            title: "Persona Agent Framework"·to_string(),
            path: "/persona/getting-started/introduction"·to_string(),
            product: "Persona"·to_string(),
            content: "Persona is a 4-layer AI agent framework built in Kotlin. \
                     It includes planning, coordination, execution, and safety layers."·to_string(),
        },
        MockDoc {
            title: "Infernum LLM Inference"·to_string(),
            path: "/infernum/getting-started/introduction"·to_string(),
            product: "Infernum"·to_string(),
            content: "Infernum is a local LLM inference engine written in Rust. \
                     It provides fast, efficient inference with minimal dependencies."·to_string(),
        },
        MockDoc {
            title: "Qliphoth Component Library"·to_string(),
            path: "/qliphoth/ui/components"·to_string(),
            product: "Qliphoth"·to_string(),
            content: "Qliphoth UI provides 40+ components styled with the Corporate Goth \
                     design system. Includes buttons, cards, modals, and specialized \
                     components like Evidence and ChatBubble."·to_string(),
        },
        MockDoc {
            title: "Morphemes in Sigil"·to_string(),
            path: "/sigil/concepts/morphemes"·to_string(),
            product: "Sigil"·to_string(),
            content: "Morphemes are functional operators in Sigil. They include \
                     τ (transform), φ (filter), σ (sort), ρ (reduce), α (first), \
                     and ω (last)."·to_string(),
        },
        MockDoc {
            title: "Evidentiality Markers"·to_string(),
            path: "/sigil/concepts/evidentiality"·to_string(),
            product: "Sigil"·to_string(),
            content: "Sigil uses evidentiality markers to encode certainty: \
                     ! for known facts, ? for uncertain values, ~ for reported \
                     information, and ‽ for paradoxes."·to_string(),
        },
    ]
}

#[derive(Clone, Debug)]
type MockDoc = struct {
    pub title: String!,
    pub path: String!,
    pub product: String!,
    pub content: String!,
}

/// Build search index from documentation
pub fn build_index(docs: &Vec[MockDoc]!) -> SearchIndex! {
    let mut index! = SearchIndex·new();

    for (doc_id, doc) in docs·iter()·enumerate() {
        // Tokenize title and content
        let title_tokens! = tokenize(&doc.title);
        let content_tokens! = tokenize(&doc.content);

        // Add to inverted index
        for token in title_tokens {
            index.inverted_index
                ·entry(token)
                ·or_insert_with(Vec·new)
                ·push(IndexEntry {
                    doc_id: doc_id as u32,
                    field: "title"·to_string(),
                    score: 2.0, // Title matches weighted higher
                });
        }

        for token in content_tokens {
            index.inverted_index
                ·entry(token)
                ·or_insert_with(Vec·new)
                ·push(IndexEntry {
                    doc_id: doc_id as u32,
                    field: "content"·to_string(),
                    score: 1.0,
                });
        }
    }

    index
}

/// Tokenize text for indexing
fn tokenize(text: &str!) -> Vec[String]! {
    text·to_lowercase()
        ·split(|c: char| !c·is_alphanumeric())
        |φ{s => !s·is_empty() && s·len() > 2!}
        |τ{s => s·to_string()}
        ·collect()
}

#[derive(Clone, Debug)]
pub type SearchIndex = struct {
    pub inverted_index: HashMap[String, Vec[IndexEntry]]!,
}

impl SearchIndex {
    pub fn new() -> Self! {
        SearchIndex {
            inverted_index: HashMap·new(),
        }
    }
}

#[derive(Clone, Debug)]
pub type IndexEntry = struct {
    pub doc_id: u32!,
    pub field: String!,
    pub score: f64!,
}
